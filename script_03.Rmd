---
title: "Week 3"
author: "David Kane"
output: html_document
---

Go to https://registrar.fas.harvard.edu/faculty-staff/courses/enrollment and scroll down to "Access the Current Course Enrollment Numbers." Click on that to download the Excel file. Create a folder in your project called `new_data`. Move the Excel file into that folder. Note that, even if you did this last week, you are doing it again because Harvard has updated the file. The file might be dated either September 21 or 22. We won't know till class!

Note that I have already created a directory called "old_data" and included the file from September 1 in it, along with other data which I have collected. Because I am your buddy, I even give you the code for reading it in! (Although I did leave one mistake for you to find . . .)

Load **tidyverse**, **readxl* and **janitor** into your setup chunk.

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(janitor)
knitr::opts_chunk$set(echo = TRUE)
```


### Scene 0

**Prompt:**  First, figure out what is wrong with the `sep_old` object. Edit the above code to fix it.

```{r sc0}
# Most groups got something like this going last week. Note the use of skip = 3
# to get rid of the garbage rows at the top of the file. Note the is.na()
# filter, which gets rid of the rows at the bottom, especially the dangerous
# summary row. Raw excel sheets are dangerous! Note that it was easy to naively
# assume that there was only one row per class. Untrue!

sep_old <- 
  read_excel("old_data/class_enrollment_summary_by_term_9-1-2020.xlsx", 
             skip = 3) %>% 
  clean_names() %>% 
  filter(! is.na(course_title)) %>% 
  select(-grad, -non_degree, -x_reg, -vus, -employee, 
         -withdraw, -total, -instructor_full_name, -course_section_code) %>%
  rename(id = course_id,
         title = course_title,
         name = course_name,
         department = course_department) %>% 
  group_by(id) %>%
  mutate(u_grad = sum(u_grad), .groups = "drop") %>%
  #filter(u_grad > 10) %>%
  distinct()


# But this is not correct! Look for Gov 50: Data. What do you see? What can you
# do to clean it up?
```




### Scene 1

**Prompt:** Read in and clean the new data, creating an object called `sep_new`. 

```{r, echo = FALSE}

sep_new <- 
  read_excel("new_data/class_enrollment_summary_by_term_9-21-2020.xlsx", 
             skip = 3) %>% 
  clean_names() %>% 
  filter(! is.na(course_title)) %>% 
  select(-grad, -non_degree, -x_reg, -vus, -employee, 
         -withdraw, -total, -instructor_full_name, -course_section_code) %>%
  rename(id = course_id,
         title = course_title,
         name = course_name,
         department = course_department) %>% 
  group_by(id) %>%
  mutate(u_grad = sum(u_grad), .groups = "drop") %>%
  #filter(u_grad > 10) %>%
  distinct()

```


### Scene 2

**Prompt:** Dean Amanda Claybaugh is concerned about the drop in undergradaute enrollment in some courses between September 1 and today. She wants you to analyze this issue. Before you dive into the details, provide some bullet points as to how Wisdom and Temperance apply to this situation. Every student should have several bullet points. Someone will be asked to share their screen and discuss.

Before we start on the model, Wisdom suggests we should:

- We want to define what we really want to know (the measurement)--the difference in undergrad enrollment between Sept 1. and now.
- We want to know why that difference exists and why we care about it?
- We have to make assumptions for what we don't want, limiting our missing data (differences between the data from days we don't have or do not care about)
- We make the assumption that all classes have the possibility of dropping above or below the 10-person threshold, so we got rid of the filter only grabbing classes with more than 10-students.

After we have a model, Temperance suggests we should:

- Understand the limitations of our data: we cannot make causational assumption from our data, or assume the same treatment effect on enrollment affects every class. 
- We also need to ensure that our data is valid externally to the population we're observing
- Moreover, we cannot introduce items of selection bias or manipulating assiggment mechanisms.
- We need to make sure our code is correct in order to ensure internal validity.

### Scene 3

**Prompt:** Which classes had the biggest increases and decreases in undergraduate enrollments between September 1 and today? Make a graphic that shows the 5 biggest increases and decreases. Make it look nice.


```{r, echo = FALSE}

sep_diff <- inner_join(sep_new, sep_old, by = "id", suffix = c(".new", ".old")) %>%
  filter(u_grad.new >= 10 & u_grad.old >= 10) %>%
  mutate(differenceen = u_grad.new - u_grad.old)

largediff <- sep_diff %>% 
  arrange(differenceen) %>%
  slice(1:5)
  
largeinc <- sep_diff %>% 
  arrange(desc(differenceen)) %>%
  slice(1:5)

```



## Scene 4

**Prompt:**  What might have caused drops in these classes? Assume that one of the causes might have been the amount of work assigned in the first two weeks of class. Create a simplified ideal Preceptor Table (using a spreadsheet of your choice) with no missing data which would allow us to investigate this situation. What data is missing and why? Create an actual Preceptor Table, again using a spreadsheet. How might we investigate the effect of work assigned in the first two weeks? Would the estimated Average Treatment Effect be accurate? Why or why not? Put some bullet points here and be prepared to show your spreadsheet to the class.


## Scene 5

Read in the data for all the available dates and use it to make a graphic which shows the changes in enrollment over time. The **gghighlight** package might be useful, perhaps to highlight what has happened in Gov 50, as compared to the other 500 or so courses.


